<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aprendiendo Machine Learning: √Årboles de Decisi√≥n y RL</title>
    <style>
        /* --- Estilos Generales --- */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f7f6;
            color: #333;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        header, footer {
            background-color: #004d40; /* Verde oscuro */
            color: #ffffff;
            text-align: center;
            padding: 1.5rem;
        }
        header h1, footer p {
            margin: 0;
        }
        h2 {
            color: #00796b; /* Verde azulado */
            border-bottom: 3px solid #00796b;
            padding-bottom: 10px;
        }
        .section {
            background-color: #ffffff;
            margin-bottom: 2rem;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .flex-container {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
        }
        .flex-child {
            flex: 1;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        /* --- Estilos para interactividad --- */
        .interactive-demo {
            background-color: #e0f2f1; /* Verde muy claro */
            padding: 1.5rem;
            border-radius: 8px;
            border: 2px dashed #00796b;
        }
        .interactive-demo h4 {
            margin-top: 0;
            color: #004d40;
        }
        select, button {
            padding: 10px 15px;
            border-radius: 5px;
            border: 1px solid #b2dfdb;
            margin-right: 10px;
            font-size: 1rem;
        }
        button {
            background-color: #00796b;
            color: white;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        button:hover {
            background-color: #004d40;
        }
        #decision-result, #rl-log {
            margin-top: 15px;
            font-weight: bold;
            color: #d84315; /* Naranja oscuro */
        }
        /* --- Estilos para la simulaci√≥n de RL --- */
        #rl-grid {
            display: grid;
            grid-template-columns: repeat(4, 50px);
            grid-template-rows: repeat(4, 50px);
            gap: 5px;
            margin-top: 15px;
            border: 2px solid #333;
            width: 215px; /* 4*50 + 3*5 */
        }
        .grid-cell {
            width: 50px;
            height: 50px;
            background-color: #e0e0e0;
            text-align: center;
            line-height: 50px;
            font-size: 1.5rem;
        }
        .agent { background-color: #42a5f5; } /* Azul */
        .reward { background-color: #66bb6a; } /* Verde */
        .danger { background-color: #ef5350; } /* Rojo */
        
        /* Media Query para responsividad */
        @media (max-width: 768px) {
            .flex-container {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>

    <header>
        <h1>Conceptos de Machine Learning en Acci√≥n</h1>
    </header>

    <main class="container">

        <section id="decision-trees" class="section">
            <h2>üå≥ √Årboles de Decisi√≥n</h2>
            <div class="flex-container">
                <div class="flex-child">
                    <p>
                        Los <strong>√Årboles de Decisi√≥n</strong> son un algoritmo de aprendizaje supervisado que funciona como un diagrama de flujo. Toman decisiones basadas en una serie de preguntas sobre los datos. Cada "nodo" interno del √°rbol es una pregunta sobre una caracter√≠stica (ej: "¬øEst√° lloviendo?"), y cada "hoja" final representa la respuesta o predicci√≥n (ej: "Jugar tenis" o "No jugar tenis").
                    </p>
                    <p>
                        Son muy populares porque son f√°ciles de interpretar y visualizar. El objetivo del algoritmo es encontrar las mejores preguntas para dividir los datos y llegar a una conclusi√≥n precisa de la manera m√°s eficiente posible.
                    </p>
                </div>
                <div class="flex-child">
                    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Decision_tree.svg/400px-Decision_tree.svg.png" alt="Diagrama de un √Årbol de Decisi√≥n">
                </div>
            </div>
            
            <div class="interactive-demo">
                <h4>Ejemplo Interactivo: ¬øDeber√≠a jugar tenis?</h4>
                <p>Selecciona las condiciones actuales y el √°rbol tomar√° una decisi√≥n.</p>
                
                <label for="clima">Clima:</label>
                <select id="clima">
                    <option value="soleado">Soleado</option>
                    <option value="nublado">Nublado</option>
                    <option value="lluvioso">Lluvioso</option>
                </select>

                <label for="humedad">Humedad:</label>
                <select id="humedad">
                    <option value="normal">Normal</option>
                    <option value="alta">Alta</option>
                </select>

                <button onclick="predecirTenis()">Predecir</button>
                <p id="decision-result"></p>
            </div>
        </section>

        <section id="reinforcement-learning" class="section">
            <h2>ü§ñ Reinforcement Learning (RL)</h2>
            <div class="flex-container">
                 <div class="flex-child">
                    <p>
                        El <strong>Aprendizaje por Refuerzo (Reinforcement Learning)</strong> es una t√©cnica donde un "agente" aprende a tomar decisiones interactuando con un "entorno". El agente recibe <strong>recompensas</strong> por realizar acciones correctas y <strong>penalizaciones</strong> por las incorrectas. Su objetivo es maximizar la recompensa total a lo largo del tiempo.
                    </p>
                    <p>
                        A diferencia de otros m√©todos, no se le dan datos etiquetados. El agente aprende por prueba y error, explorando el entorno y descubriendo qu√© acciones conducen a los mejores resultados. Es la base de muchos sistemas de IA en rob√≥tica, juegos y optimizaci√≥n.
                    </p>
                </div>
                <div class="flex-child">
                    <img src="https://www.researchgate.net/profile/Irina-Cherunova/publication/336943882/figure/fig1/AS:820481028304897@1572635928452/The-general-scheme-of-the-reinforcement-learning-process-A-reinforcement-learning.png" alt="Diagrama de Reinforcement Learning">
                </div>
            </div>

            <div class="interactive-demo">
                <h4>Simulaci√≥n: El Agente en la Cuadr√≠cula</h4>
                <p>El agente (üü¶) debe aprender a llegar a la recompensa (‚úÖ) evitando el peligro (‚ùå). Cada vez que ejecutas un "episodio", el agente explora y actualiza su estrategia. Observa c√≥mo encuentra el camino m√°s r√°pido con el tiempo.</p>
                
                <button onclick="runRLEpisode()">Ejecutar 1 Episodio</button>
                <button onclick="resetRL()">Reiniciar Simulaci√≥n</button>
                
                <div id="rl-grid"></div>
                <div id="rl-log">Estado: Listo para empezar.</div>
            </div>
        </section>

    </main>

    <footer>
        <p>Proyecto Did√°ctico de Machine Learning - 2024</p>
    </footer>

    <script>
        // --- L√≥gica para el √Årbol de Decisi√≥n ---
        function predecirTenis() {
            const clima = document.getElementById('clima').value;
            const humedad = document.getElementById('humedad').value;
            const resultado = document.getElementById('decision-result');

            let decision = '';

            // L√≥gica simple del √°rbol de decisi√≥n
            if (clima === 'soleado') {
                if (humedad === 'alta') {
                    decision = '‚ùå No, mala idea jugar tenis (demasiada humedad).';
                } else {
                    decision = '‚úÖ ¬°S√≠! Es un d√≠a perfecto para jugar tenis.';
                }
            } else if (clima === 'nublado') {
                decision = '‚úÖ S√≠, es un buen d√≠a para jugar tenis.';
            } else if (clima === 'lluvioso') {
                decision = '‚ùå No, no se puede jugar tenis con lluvia.';
            }
            
            resultado.textContent = `Decisi√≥n: ${decision}`;
        }

        // --- L√≥gica para Reinforcement Learning ---
        const gridSize = 4;
        const gridElement = document.getElementById('rl-grid');
        const rlLog = document.getElementById('rl-log');

        const agentChar = 'üü¶';
        const rewardChar = '‚úÖ';
        const dangerChar = '‚ùå';

        let agentPos = { x: 0, y: 0 };
        const rewardPos = { x: 3, y: 3 };
        const dangerPos = { x: 2, y: 1 };
        
        // Q-table: Almacena el valor aprendido para cada acci√≥n en cada estado
        // Simplificado: aqu√≠ no implementaremos una Q-table completa, sino que simularemos el movimiento
        let episodeCount = 0;

        // Funci√≥n para dibujar la cuadr√≠cula
        function drawGrid() {
            gridElement.innerHTML = '';
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    const cell = document.createElement('div');
                    cell.classList.add('grid-cell');
                    if (x === agentPos.x && y === agentPos.y) {
                        cell.textContent = agentChar;
                        cell.classList.add('agent');
                    } else if (x === rewardPos.x && y === rewardPos.y) {
                        cell.textContent = rewardChar;
                        cell.classList.add('reward');
                    } else if (x === dangerPos.x && y === dangerPos.y) {
                        cell.textContent = dangerChar;
                        cell.classList.add('danger');
                    }
                    gridElement.appendChild(cell);
                }
            }
        }

        // Funci√≥n para reiniciar la simulaci√≥n
        function resetRL() {
            agentPos = { x: 0, y: 0 };
            episodeCount = 0;
            rlLog.textContent = 'Simulaci√≥n reiniciada. Listo para empezar.';
            drawGrid();
        }

        // Simula un episodio de RL
        function runRLEpisode() {
            episodeCount++;
            rlLog.textContent = `Episodio ${episodeCount}: El agente est√° explorando...`;
            agentPos = { x: 0, y: 0 }; // Reiniciar posici√≥n del agente
            drawGrid();

            let moves = 0;
            const maxMoves = 20;

            const interval = setInterval(() => {
                if (moves >= maxMoves || (agentPos.x === rewardPos.x && agentPos.y === rewardPos.y)) {
                    clearInterval(interval);
                    if (agentPos.x === rewardPos.x && agentPos.y === rewardPos.y) {
                         rlLog.textContent = `Episodio ${episodeCount}: ¬°Recompensa encontrada en ${moves} movimientos!`;
                    } else {
                         rlLog.textContent = `Episodio ${episodeCount}: L√≠mite de movimientos alcanzado. Intentando de nuevo.`;
                    }
                    return;
                }

                // L√≥gica de movimiento simple (no es Q-learning real, sino una simulaci√≥n)
                // El agente intenta moverse hacia la recompensa
                const directionX = Math.sign(rewardPos.x - agentPos.x);
                const directionY = Math.sign(rewardPos.y - agentPos.y);
                
                let nextX = agentPos.x;
                let nextY = agentPos.y;

                // Moverse en X o Y aleatoriamente para simular exploraci√≥n
                if (Math.random() > 0.5) {
                    nextX += directionX;
                } else {
                    nextY += directionY;
                }
                
                // Validar que no se salga de la cuadr√≠cula y que no caiga en el peligro
                if (nextX >= 0 && nextX < gridSize && nextY >= 0 && nextY < gridSize && !(nextX === dangerPos.x && nextY === dangerPos.y)) {
                    agentPos.x = nextX;
                    agentPos.y = nextY;
                }
                
                moves++;
                drawGrid();
            }, 200); // 200ms por movimiento
        }

        // Dibujar el estado inicial al cargar la p√°gina
        window.onload = () => {
            drawGrid();
        };
    </script>
</body>
</html>